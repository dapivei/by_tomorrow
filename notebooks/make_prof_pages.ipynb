{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49d07c2-07b6-404b-b24e-ba912e618f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import io\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "import dr_util.file_utils as fu\n",
    "import bytom.author_profiles as ap\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f5a7d5-7212-4a6d-aa31-452de8f4a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_path = \"../configs/\"\n",
    "with initialize(config_path=conf_path, version_base=None):\n",
    "    os.makedirs(os.path.dirname(conf_path), exist_ok=True)\n",
    "    cfg = compose(config_name=\"paper_data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b35dd13-b692-45f5-a561-9b6231f5262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: \n",
      "\n",
      "data_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/\n",
      "raw_pdf_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/raw_pdfs/\n",
      "parsed_pdf_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/parsed_pdfs/\n",
      "metadata_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/parsed_pdfs/\n",
      "author_data_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/author_data/\n",
      "author_summaries_dir: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/author_data/summaries/\n",
      "author_info_file: /Users/danielapintoveizaga/Documents/github/by_tomorrow/data/author_data/manual_profiles.json\n",
      "prof_pattern: (?P<professor_name>[\\w_]+)\n",
      "file_type_pattern: (?P<file_type>\\w+)\n",
      "version_pattern: v(?P<version>\\d+)\n",
      "author_summary_file_pattern: (?P<professor_name>[\\w_]+)\\.(?P<file_type>\\w+)\\.v(?P<version>\\d+)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHANGE the data dir path\n",
    "cfg.data_dir = \"/Users/danielapintoveizaga/Documents/github/by_tomorrow/data/\"\n",
    "cfg_resolved = OmegaConf.to_container(cfg, resolve=True)\n",
    "print(f\"Configuration: \\n\\n{OmegaConf.to_yaml(cfg_resolved)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbad9330-878f-449f-8f1a-f8f6c65a883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE this\n",
    "\n",
    "names_info = {\n",
    "    \"Aviad Levis\": {\n",
    "        \"bio\": \"\"\n",
    "    },\n",
    "    \"Doug Downey\": {\n",
    "        \"bio\": \"\"\n",
    "    },\n",
    "    \"Heng Ji\": {\n",
    "        \"bio\": \"\"\n",
    "    },\n",
    "    \"Jeff Clune\": {\n",
    "        \"bio\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#Call the function\n",
    "ap.save_info_json(cfg, names_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054ca560-24b4-456d-a35b-0019f10a37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_INFO = fu.load_file(cfg.author_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fe30e6-60e7-4376-962d-8fa999481a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:\n",
      " - Aviad Levis\n",
      " - Doug Downey\n",
      " - Heng Ji\n",
      " - Jeff Clune\n"
     ]
    }
   ],
   "source": [
    "print(\"Authors:\")\n",
    "for k in AUTHOR_INFO.keys():\n",
    "    print(f\" - {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5ed6c4-e97c-4582-bf59-96f409231f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aviad = ap.get_author_papers('Heng Ji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba2b3eb-c35b-463f-812e-770bcabc493d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Scaling Laws for Predicting Downstream Performance in LLMs',\n",
       "  'abstract': 'Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical \"emergent phase\". In preliminary experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. This motivates FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpora with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins.',\n",
       "  'published': '2024-10-11',\n",
       "  'updated': '2024-10-11',\n",
       "  'authors': ['Yangyi Chen',\n",
       "   'Binxuan Huang',\n",
       "   'Yifan Gao',\n",
       "   'Zhengyang Wang',\n",
       "   'Jingfeng Yang',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2410.08527v1'},\n",
       " {'title': 'MentalArena: Self-play Training of Language Models for Diagnosis and\\n  Treatment of Mental Health Disorders',\n",
       "  'abstract': 'Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main',\n",
       "  'published': '2024-10-09',\n",
       "  'updated': '2024-10-09',\n",
       "  'authors': ['Cheng Li',\n",
       "   'May Fung',\n",
       "   'Qingyun Wang',\n",
       "   'Chi Han',\n",
       "   'Manling Li',\n",
       "   'Jindong Wang',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2410.06845v1'},\n",
       " {'title': 'Self-Correction is More than Refinement: A Learning Framework for Visual\\n  and Language Reasoning Tasks',\n",
       "  'abstract': 'While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.',\n",
       "  'published': '2024-10-05',\n",
       "  'updated': '2024-10-05',\n",
       "  'authors': ['Jiayi He', 'Hehai Lin', 'Qingyun Wang', 'Yi Fung', 'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2410.04055v1'},\n",
       " {'title': 'Aligning LLMs with Individual Preferences via Interaction',\n",
       "  'abstract': \"As large language models (LLMs) demonstrate increasingly advanced capabilities, aligning their behaviors with human values and preferences becomes crucial for their wide adoption. While previous research focuses on general alignment to principles such as helpfulness, harmlessness, and honesty, the need to account for individual and diverse preferences has been largely overlooked, potentially undermining customized human experiences. To address this gap, we train LLMs that can ''interact to align'', essentially cultivating the meta-skill of LLMs to implicitly infer the unspoken personalized preferences of the current user through multi-turn conversations, and then dynamically align their following behaviors and responses to these inferred preferences. Our approach involves establishing a diverse pool of 3,310 distinct user personas by initially creating seed examples, which are then expanded through iterative self-generation and filtering. Guided by distinct user personas, we leverage multi-LLM collaboration to develop a multi-turn preference dataset containing 3K+ multi-turn conversations in tree structures. Finally, we apply supervised fine-tuning and reinforcement learning to enhance LLMs using this dataset. For evaluation, we establish the ALOE (ALign With CustOmized PrEferences) benchmark, consisting of 100 carefully selected examples and well-designed metrics to measure the customized alignment performance during conversations. Experimental results demonstrate the effectiveness of our method in enabling dynamic, personalized alignment via interaction.\",\n",
       "  'published': '2024-10-04',\n",
       "  'updated': '2024-10-04',\n",
       "  'authors': ['Shujin Wu',\n",
       "   'May Fung',\n",
       "   'Cheng Qian',\n",
       "   'Jeonghwan Kim',\n",
       "   'Dilek Hakkani-Tur',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2410.03642v1'},\n",
       " {'title': 'FARM: Functional Group-Aware Representations for Small Molecules',\n",
       "  'abstract': \"We introduce Functional Group-Aware Representations for Small Molecules (FARM), a novel foundation model designed to bridge the gap between SMILES, natural language, and molecular graphs. The key innovation of FARM lies in its functional group-aware tokenization, which directly incorporates functional group information into the representations. This strategic reduction in tokenization granularity is intentionally aligned with key drivers of functional properties (i.e., functional groups), enhancing the model's understanding of chemical language. By expanding the chemical lexicon, FARM more effectively bridges SMILES and natural language, ultimately advancing the model's capacity to predict molecular properties. FARM also represents molecules from two perspectives: by using masked language modeling to capture atom-level features and by employing graph neural networks to encode the whole molecule topology. By leveraging contrastive learning, FARM aligns these two views of representations into a unified molecular embedding. We rigorously evaluate FARM on the MoleculeNet dataset, where it achieves state-of-the-art performance on 10 out of 12 tasks. These results highlight FARM's potential to improve molecular representation learning, with promising applications in drug discovery and pharmaceutical research.\",\n",
       "  'published': '2024-10-02',\n",
       "  'updated': '2024-10-06',\n",
       "  'authors': ['Thao Nguyen',\n",
       "   'Kuan-Hao Huang',\n",
       "   'Ge Liu',\n",
       "   'Martin D. Burke',\n",
       "   'Ying Diao',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2410.02082v2'},\n",
       " {'title': 'Search and Detect: Training-Free Long Tail Object Detection via\\n  Web-Image Retrieval',\n",
       "  'abstract': 'In this paper, we introduce SearchDet, a training-free long-tail object detection framework that significantly enhances open-vocabulary object detection performance. SearchDet retrieves a set of positive and negative images of an object to ground, embeds these images, and computes an input image-weighted query which is used to detect the desired concept in the image. Our proposed method is simple and training-free, yet achieves over 48.7% mAP improvement on ODinW and 59.1% mAP improvement on LVIS compared to state-of-the-art models such as GroundingDINO. We further show that our approach of basing object detection on a set of Web-retrieved exemplars is stable with respect to variations in the exemplars, suggesting a path towards eliminating costly data annotation and training procedures.',\n",
       "  'published': '2024-09-26',\n",
       "  'updated': '2024-09-26',\n",
       "  'authors': ['Mankeerat Sidhu',\n",
       "   'Hetarth Chopra',\n",
       "   'Ansel Blume',\n",
       "   'Jeonghwan Kim',\n",
       "   'Revanth Gangi Reddy',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2409.18733v1'},\n",
       " {'title': 'Towards LifeSpan Cognitive Systems',\n",
       "  'abstract': 'Building a human-like system that continuously interacts with complex environments -- whether simulated digital worlds or human society -- presents several key challenges. Central to this is enabling continuous, high-frequency interactions, where the interactions are termed experiences. We refer to this envisioned system as the LifeSpan Cognitive System (LSCS). A critical feature of LSCS is its ability to engage in incremental and rapid updates while retaining and accurately recalling past experiences. We identify two major challenges in achieving this: (1) Abstraction and Experience Merging, and (2) Long-term Retention with Accurate Recall. These properties are essential for storing new experiences, organizing past experiences, and responding to the environment in ways that leverage relevant historical data. Unlike language models with continual learning, which typically rely on large corpora for fine-tuning and focus on improving performance within specific domains or tasks, LSCS must rapidly and incrementally update with new information from its environment at a high frequency. Existing technologies with the potential of solving the above two major challenges can be classified into four classes based on a conceptual metric called Storage Complexity, which measures the relative space required to store past experiences. Each of these four classes of technologies has its own strengths and limitations. Given that none of the existing technologies can achieve LSCS alone, we propose a novel paradigm for LSCS that integrates all four classes of technologies. The new paradigm operates through two core processes: Absorbing Experiences and Generating Responses.',\n",
       "  'published': '2024-09-20',\n",
       "  'updated': '2024-09-20',\n",
       "  'authors': ['Yu Wang',\n",
       "   'Chi Han',\n",
       "   'Tongtong Wu',\n",
       "   'Xiaoxin He',\n",
       "   'Wangchunshu Zhou',\n",
       "   'Nafis Sadeq',\n",
       "   'Xiusi Chen',\n",
       "   'Zexue He',\n",
       "   'Wei Wang',\n",
       "   'Gholamreza Haffari',\n",
       "   'Heng Ji',\n",
       "   'Julian McAuley'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2409.13265v1'},\n",
       " {'title': 'PropaInsight: Toward Deeper Understanding of Propaganda in Terms of\\n  Techniques, Appeals, and Intent',\n",
       "  'abstract': 'Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce propainsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. propainsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present propagaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline. Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but training with propagaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis.',\n",
       "  'published': '2024-09-19',\n",
       "  'updated': '2024-09-19',\n",
       "  'authors': ['Jiateng Liu',\n",
       "   'Lin Ai',\n",
       "   'Zizhou Liu',\n",
       "   'Payam Karisani',\n",
       "   'Zheng Hui',\n",
       "   'May Fung',\n",
       "   'Preslav Nakov',\n",
       "   'Julia Hirschberg',\n",
       "   'Heng Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2409.18997v1'},\n",
       " {'title': 'Automating Knowledge Discovery from Scientific Literature via LLMs: A\\n  Dual-Agent Approach with Progressive Ontology Prompting',\n",
       "  'abstract': 'To address the challenge of automating knowledge discovery from a vast volume of literature, in this paper, we introduce a novel framework based on large language models (LLMs) that combines a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo, designed to enhance the automation of knowledge extraction from scientific articles. The POP algorithm utilizes a prioritized breadth-first search (BFS) across a predefined ontology to generate structured prompt templates and action orders, thereby guiding LLMs to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs two specialized LLM agents: an explorer and an evaluator. These two agents work collaboratively and adversarially to enhance the reliability of the discovery and annotation processes. Experiments demonstrate that our method outperforms advanced baselines, enabling more accurate and complete annotations. To validate the effectiveness of our method in real-world scenarios, we employ our method in a case study of speech-language intervention discovery. Our method identifies 2,421 interventions from 64,177 research articles in the speech-language therapy domain. We curate these findings into a publicly accessible intervention knowledge base that holds significant potential to benefit the speech-language therapy community.',\n",
       "  'published': '2024-08-20',\n",
       "  'updated': '2024-08-20',\n",
       "  'authors': ['Yuting Hu',\n",
       "   'Dancheng Liu',\n",
       "   'Qingyun Wang',\n",
       "   'Charles Yu',\n",
       "   'Heng Ji',\n",
       "   'Jinjun Xiong'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2409.00054v1'},\n",
       " {'title': 'Geometry Informed Tokenization of Molecules for Language Model\\n  Generation',\n",
       "  'abstract': 'We consider molecule generation in 3D space using language models (LMs), which requires discrete tokenization of 3D molecular geometries. Although tokenization of molecular graphs exists, that for 3D geometries is largely unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which converts molecular geometries into $SE(3)$-invariant 1D discrete sequences. Geo2Seq consists of canonical labeling and invariant spherical representation steps, which together maintain geometric and atomic fidelity in a format conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various LMs excel in molecular geometry generation, especially in controlled generation tasks.',\n",
       "  'published': '2024-08-19',\n",
       "  'updated': '2024-08-19',\n",
       "  'authors': ['Xiner Li',\n",
       "   'Limei Wang',\n",
       "   'Youzhi Luo',\n",
       "   'Carl Edwards',\n",
       "   'Shurui Gui',\n",
       "   'Yuchao Lin',\n",
       "   'Heng Ji',\n",
       "   'Shuiwang Ji'],\n",
       "  'pdf_link': 'http://arxiv.org/pdf/2408.10120v1'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aviad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cffab03e-f588-4fd1-a062-594ce081ac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Title:** MentalArena: Self-play Training of Language Models for Diagnosis and\n",
       "  Treatment of Mental Health Disorders\n",
       "\n",
       "**Publish Date:** 2024-10-09\n",
       "\n",
       "**First Author:** Cheng Li\n",
       "\n",
       "**Last Author:** Heng Ji\n",
       "\n",
       "**Middle Authors:** May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang\n",
       "\n",
       "**Abstract:** Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main\n",
       "\n",
       "---------------\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(ap.format_response_abstract_to_markdown(aviad[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68af941-3811-44d7-9bb2-968dcce380dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895fde6f-fa2e-46d3-a5cd-d5b98349a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.write_author_page(\n",
    "    cfg, \"Heng Ji\", version='1',\n",
    "    max_papers=100,\n",
    "    max_years=5,\n",
    "    first_last_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcfdebf-098b-42eb-aa1a-b693b60e25c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Summary for **Heng Ji**\n",
       "\n",
       "## Heng Ji Bio\n",
       "\n",
       "{'bio': ''}\n",
       "\n",
       "## Recent Papers\n",
       "\n",
       "### **Title:** Scaling Laws for Predicting Downstream Performance in LLMs\n",
       "\n",
       "**Publish Date:** 2024-10-11\n",
       "\n",
       "**First Author:** Yangyi Chen\n",
       "\n",
       "**Last Author:** Heng Ji\n",
       "\n",
       "**Middle Authors:** Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang\n",
       "\n",
       "**Abstract:** Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical \"emergent phase\". In preliminary experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. This motivates FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpora with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins.\n",
       "\n",
       "---------------\n",
       "\n",
       "\n",
       "### **Title:** MentalArena: Self-play Training of Language Models for Diagnosis and\n",
       "  Treatment of Mental Health Disorders\n",
       "\n",
       "**Publish Date:** 2024-10-09\n",
       "\n",
       "**First Author:** Cheng Li\n",
       "\n",
       "**Last Author:** Heng Ji\n",
       "\n",
       "**Middle Authors:** May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang\n",
       "\n",
       "**Abstract:** Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main\n",
       "\n",
       "---------------\n",
       "\n",
       "\n",
       "### **Title:** Self-Correction is More than Refinement: A Learning Framework for Visual\n",
       "  and Language Reasoning Tasks\n",
       "\n",
       "**Publish Date:** 2024-10-05\n",
       "\n",
       "**First Author:** Jiayi He\n",
       "\n",
       "**Last Author:** Heng Ji\n",
       "\n",
       "**Middle Authors:** Hehai Lin, Qingyun Wang, Yi Fung\n",
       "\n",
       "**Abstract:** While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.\n",
       "\n",
       "---------------\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "    ap.make_author_page(\n",
    "        cfg, \"Heng Ji\", max_papers=3, max_years=5, first_last_only=True,\n",
    "    )\n",
    "))\n",
    "#fu.dump_file(author_page, f\"{cfg.author_summaries_dir}he_he.markdown.v1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73133a-fa07-42f6-b8c1-695d5dfbf9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba607c11-d36f-4ab5-a97a-9eb28b181b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b926e4-44a6-4889-8229-4faef6cfcd84",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316ed9e-3ceb-47d1-ad3e-3f5389c47f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_metadata_path(author):\n",
    "    assert author in AUTHORS\n",
    "    return f'{METADATA_DIR}{author.replace(\" \", \"_\")}_query_metadata.json'\n",
    "\n",
    "def get_author_metadata(author):\n",
    "    md_path = get_author_metadata_path(author)\n",
    "    md = fu.load_file(md_path)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b988a0c-3014-41c8-ad48-57005996e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_pdf_path(pdf_name):\n",
    "    return f'{PARSED_PDF_DIR}{pdf_name}.pkl'\n",
    "\n",
    "def get_parsed_pdf(pdf_name):\n",
    "    ppdf_path = get_parsed_pdf_path(pdf_name)\n",
    "    if os.path.exists(ppdf_path):\n",
    "        return fu.load_file(ppdf_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afc086-f402-47df-b1b6-2703aa2b116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_parsed_papers(author):\n",
    "    md = get_author_metadata(author)\n",
    "    pdfs_dict = md['pdfs_metadata']\n",
    "    parsed_pdfs_dict = []\n",
    "    for pdf_name, pdf_data in pdfs_dict.items():\n",
    "        ppdf = get_parsed_pdf(pdf_name)\n",
    "        if ppdf is None:\n",
    "            continue\n",
    "        ppdf_dict = {**pdf_data}\n",
    "        ppdf_dict['parsed_pdf'] = ppdf\n",
    "        parsed_pdfs_dict.append(ppdf_dict)\n",
    "    return parsed_pdfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3d56f-3a75-4a37-ad4a-6cf87a08d719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fd6f21-761e-4c03-8c9f-9e51b4a04ede",
   "metadata": {},
   "source": [
    "## Load Parsed, Extract Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff237-12ca-4e0d-9671-8cd97a09b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pdfs_pavel = get_author_parsed_papers(AUTHORS[0])\n",
    "print(f\">> Number of parsed papers for {AUTHORS[0]}: {len(parsed_pdfs_pavel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7c63b-e202-4bde-b72e-7d5da6dd2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppdf = parsed_pdfs_pavel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5569bc-dc4f-4f9c-be6c-a3d7f5d55a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ppdf['title'])\n",
    "print(test_ppdf['published'])\n",
    "print(test_ppdf['authors'])\n",
    "print(test_ppdf['pdf_link'])\n",
    "print(f\">> Num blocks in parsed pdf: {len(test_ppdf['parsed_pdf'])}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c9cf06d-cd84-4506-9ba5-ecdcc4be3be2",
   "metadata": {},
   "source": [
    "for i, bl in enumerate(test_ppdf['parsed_pdf'][:5]):\n",
    "    print(f\">> Block: {i}\")\n",
    "    print(bl.text)\n",
    "    print(\"\\n\\n---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b932264-8ae6-4380-97df-06a52925e4d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e046df6-8721-4525-97b2-461708beb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_split_text(split_text, verbose=False):\n",
    "    buff = io.StringIO()\n",
    "    for section in split_text:\n",
    "        if verbose:\n",
    "            buff.write(f\"\\n\\n ===== Heading: {section['heading']} \\n\\n\")\n",
    "        buff.write(\"\\n\\n\".join(section['lines']))\n",
    "        buff.write(\"\\n\\n\")\n",
    "    return buff.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25328508-e9c5-4ca8-860b-9bc63aba038b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ab4ec-167b-46a1-a173-54458130d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_heading(text, title):\n",
    "    tls = text.split(\"\\n\")\n",
    "    title_str = f\"# {title}\"\n",
    "    sections = []\n",
    "\n",
    "    start_tl_strip = tls[0].strip()\n",
    "    if title_str in start_tl_strip or start_tl_strip[0] != \"#\":\n",
    "        start_heading = \"From Previous Block\"\n",
    "        start_lines = []\n",
    "    else:\n",
    "        start_heading = start_tl_strip[2:]\n",
    "        start_lines = [start_tl_strip]\n",
    "        \n",
    "    curr_section = {\"heading\": start_heading, \"lines\": start_lines}\n",
    "    for tl in tls[1:]:\n",
    "        tl_strip = tl.strip()\n",
    "        if len(tl_strip) == 0 or tl_strip[0].isdigit():\n",
    "            continue\n",
    "\n",
    "        if tl_strip[0] == \"#\":\n",
    "            # Drop all header mentions of the title, we'll add it back in\n",
    "            if title_str in tl_strip:\n",
    "                continue\n",
    "            # Otherwise start a new section\n",
    "            sections.append(curr_section)\n",
    "            curr_section = {\"heading\": tl_strip[2:], \"lines\": []}\n",
    "        curr_section['lines'].append(tl_strip)\n",
    "    \n",
    "    sections.append(curr_section)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2c528-bed2-474b-8937-fc681ba463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sects(input_ppdf, input_title):\n",
    "    all_sects = []\n",
    "    for i, block in enumerate(input_ppdf):\n",
    "        sects = split_by_heading(block.text, input_title)\n",
    "        if i == 0:\n",
    "            # Drop the title section\n",
    "            all_sects.extend(sects[1:])\n",
    "        else:\n",
    "            all_sects.extend(sects)\n",
    "    return all_sects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885add2-adb7-4bc2-b2c4-0d609b0bc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sections(sections):\n",
    "    grouped_sections = []\n",
    "\n",
    "    figs = []\n",
    "    last_was_fig = False\n",
    "    for section in sections:\n",
    "        if len(section['lines']) == 0:\n",
    "            continue\n",
    "            \n",
    "        heading = section['heading']\n",
    "        \n",
    "        # For ease of reading split the starting case out\n",
    "        if len(grouped_sections) == 0:\n",
    "            grouped_sections.append({\n",
    "                'heading': heading,\n",
    "                'lines': [],\n",
    "            })\n",
    "            \n",
    "        if heading.startswith(\"Figure\"):\n",
    "            figs.append(section)\n",
    "            last_was_fig = True\n",
    "            continue\n",
    "\n",
    "        if last_was_fig:\n",
    "            last_was_fig = False\n",
    "            if len(section['lines']) == 0:\n",
    "                print(section)\n",
    "                assert False\n",
    "            if len(section['lines'][0]) == 0:\n",
    "                print(section)\n",
    "                assert False\n",
    "            if section['lines'][0][0].islower():\n",
    "                first_l = f\"{section['heading']} {section['lines'][0]}\"\n",
    "                grouped_sections[-1]['lines'].append(first_l)\n",
    "                grouped_sections[-1]['lines'].extend(section['lines'][1:])\n",
    "                continue\n",
    "        \n",
    "        if (heading != \"From Previous Block\" and\n",
    "            grouped_sections[-1]['heading'] != heading\n",
    "        ):\n",
    "            grouped_sections.append({\n",
    "                'heading': heading,\n",
    "                'lines': [],\n",
    "            })\n",
    "        grouped_sections[-1]['lines'].extend(section['lines'])    \n",
    "    return grouped_sections, figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960080-b716-4293-872b-2fab5b5ed7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppdf_to_body_refs_figs(input_ppdf):\n",
    "    all_s = get_all_sects(input_ppdf['parsed_pdf'], input_ppdf['title'])\n",
    "    print(f\">> There are {len(all_s)} sections total.\")\n",
    "\n",
    "    grouped_s, figs_s = group_sections(all_s)\n",
    "    print(f\">> There are {len(grouped_s)} grouped sections and {len(figs_s)} figures.\")\n",
    "\n",
    "    body_s = []\n",
    "    references = None\n",
    "    for s in grouped_s:\n",
    "        if 'References' in s['heading']:\n",
    "            references = s\n",
    "            break\n",
    "        body_s.append(s)\n",
    "    return body_s, figs_s, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0dfb6-b9a6-4a40-90e9-9b088ba530b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337933b1-c2e4-477e-8165-b95337bc128c",
   "metadata": {},
   "source": [
    "## Test Full Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537ac0b-1978-4788-96b2-9b80635da4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_s, fg_s, rfs = ppdf_to_body_refs_figs(test_ppdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9ad8a-c897-40a7-ad5b-8e5f098a3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reconstruct_split_text(bd_s + fg_s))\n",
    "# rfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0cd93-a201-4f69-a0b5-80865047f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt in bd_s:\n",
    "    print(f\"{len(gt['lines']):2} | {gt['heading']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c57118-1487-490d-96e7-4ce730fb0f72",
   "metadata": {},
   "source": [
    "### Sub Section Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3d39f-9016-40ef-8fae-367f6fb140d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_test = get_all_sects(test_ppdf['parsed_pdf'], test_ppdf['title'])\n",
    "print(f\">> There are {len(all_sections_test)} sections total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b6f70-43e9-4bc1-8d5f-0d23d3c354f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test, figs_test = group_sections(all_sections_test)\n",
    "print(f\">> There are {len(grouped_test)} grouped sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b964db-3d75-4789-8193-b471a61dd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt in grouped_test:\n",
    "    print(f\"{len(gt['lines']):3} | {gt['heading']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef9303-028d-4159-aec1-8e4425f7435c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d74c3b-05ac-43e8-a58e-9bc30529bba6",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91a50b-9fd0-4731-9af6-fb8d21d1553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_author_page(author):\n",
    "    bio = AUTHOR_INFO[author]\n",
    "    \n",
    "    buff = io.StringIO()\n",
    "    buff.write(f\"# Research Summary for {author}\\n\\n\")\n",
    "    buff.write(f\"## Bio\\n{bio}\\n\\n\")\n",
    "    \n",
    "\n",
    "    buff.write(\"## Recent Papers\\n\\n\")\n",
    "    parsed_pdfs_author = get_author_parsed_papers(author)\n",
    "    for ppdf in parsed_pdfs_author:\n",
    "        buff.write(f\"# Title: {ppdf['title']}\\n Published: {ppdf['published']}\\n\")\n",
    "        buff.write(\"Authors: \" + \", \".join(ppdf['authors']) + \"\\n\\n\")\n",
    "\n",
    "        bd_s, fg_s, rfs = ppdf_to_body_refs_figs(ppdf)\n",
    "        buff.write(reconstruct_split_text(bd_s + fg_s))# + [rfs]))\n",
    "        buff.write(f\"\\n\\n -------------- End Paper: {ppdf['title']}\")\n",
    "    return buff.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d69b8-ce90-4582-8a40-98225ed26b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_and_one_paper = make_author_page(AUTHORS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb898865-80ae-4204-975f-e32d10b90158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu.dump_file(bio_and_one_paper, '/Users/daniellerothermel/drotherm/data/pavel_izmailov_summary_markdown.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4720f-192e-4405-9b4f-a56bf39b18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in AUTHORS:\n",
    "    bio_and_one_paper = make_author_page(author)\n",
    "    fu.dump_file(bio_and_one_paper, f'/Users/daniellerothermel/drotherm/data/{author.replace(\" \", \"_\").lower()}_summary_markdown.txt', verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650845e-e4aa-48fe-a920-7321c777f348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
