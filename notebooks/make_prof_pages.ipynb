{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49d07c2-07b6-404b-b24e-ba912e618f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import io\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "\n",
    "import dr_util.file_utils as fu\n",
    "import bytom.author_profiles as ap\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ee9b0-2486-48f0-ad30-27603fcba942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f5a7d5-7212-4a6d-aa31-452de8f4a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../configs/\", version_base=None):\n",
    "    cfg = compose(config_name=\"paper_data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6dbe29c-c331-4bb4-9da2-8ac0fb59dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: \n",
      "\n",
      "data_dir: /Users/daniellerothermel/drotherm/data/\n",
      "raw_pdf_dir: /Users/daniellerothermel/drotherm/data/raw_pdfs/\n",
      "parsed_pdf_dir: /Users/daniellerothermel/drotherm/data/parsed_pdfs/\n",
      "metadata_dir: /Users/daniellerothermel/drotherm/data/parsed_pdfs/\n",
      "author_data_dir: /Users/daniellerothermel/drotherm/data/author_data/\n",
      "author_summaries_dir: /Users/daniellerothermel/drotherm/data/author_data/summaries/\n",
      "author_info_file: /Users/daniellerothermel/drotherm/data/author_data/manual_profiles.json\n",
      "prof_pattern: (?P<professor_name>[\\w_]+)\n",
      "file_type_pattern: (?P<file_type>\\w+)\n",
      "version_pattern: v(?P<version>\\d+)\n",
      "author_summary_file_pattern: (?P<professor_name>[\\w_]+)\\.(?P<file_type>\\w+)\\.v(?P<version>\\d+)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_resolved = OmegaConf.to_container(cfg, resolve=True)\n",
    "print(f\"Configuration: \\n\\n{OmegaConf.to_yaml(cfg_resolved)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902af3a-d35b-45f2-a02f-e5495ed231b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054ca560-24b4-456d-a35b-0019f10a37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_INFO = fu.load_file(cfg.author_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fe30e6-60e7-4376-962d-8fa999481a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:\n",
      " - He He\n",
      " - Eunsol Choi\n",
      " - Mengye Ren\n",
      " - Rajesh Ranganath\n",
      " - Tal Linzen\n",
      " - Kyunghyun Cho\n",
      " - Lerrel Pinto\n",
      " - Pavel Izmailov\n"
     ]
    }
   ],
   "source": [
    "print(\"Authors:\")\n",
    "for k in AUTHOR_INFO.keys():\n",
    "    print(f\" - {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5ed6c4-e97c-4582-bf59-96f409231f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "he_papers = ap.get_author_papers('He He')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cffab03e-f588-4fd1-a062-594ce081ac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Title:** GOODAT: Towards Test-time Graph Out-of-Distribution Detection\n",
       "\n",
       "**Publish Date:** 2024-01-10\n",
       "\n",
       "**First Author:** Luzhi Wang\n",
       "\n",
       "**Last Author:** Tat-Seng Chua\n",
       "\n",
       "**Middle Authors:** Dongxiao He, He Zhang, Yixin Liu, Wenjie Wang, Shirui Pan, Di Jin\n",
       "\n",
       "**Abstract:** Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets. The code is available at Github: https://github.com/Ee1s/GOODAT\n",
       "\n",
       "---------------\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(ap.format_response_abstract_to_markdown(he_papers[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c51cd1-18d5-4dc8-aac0-36fefdbc475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "895fde6f-fa2e-46d3-a5cd-d5b98349a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Author Page\n",
    "ap.write_author_page(\n",
    "    cfg, \"He He\", '1',\n",
    "    max_papers=100,\n",
    "    max_years=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68af941-3811-44d7-9bb2-968dcce380dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bcfdebf-098b-42eb-aa1a-b693b60e25c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Summary for **He He**\n",
       "\n",
       "## He He Bio\n",
       "\n",
       "\n",
       "Assistant Professor of Computer Science and Data Science\n",
       "\n",
       "Bio: He He is an Assistant Professor in Computer Science and Data Science. She is broadly interested in natural language process and machine learning. Her recent research focuses on understanding large language models, improving their trustworthiness, and human-AI interaction. Prior to joining NYU, she obtained her PhD from University of Maryland, did a post-doc at Stanford, and spent one year at AWS working in dialogue platforms.\n",
       "\n",
       "Research Areas:\n",
       "\n",
       "- Machine learning\n",
       "- Deep learning\n",
       "- Natural language processing\n",
       "\n",
       "I want to build intelligent systems that can communicate with humans effectively and enable individuals to achieve their goals. Today’s systems are often opaque, brittle, and difficult to control, which limits their usefulness in human-centered applications. To make them our trustworthy collaborators, my research aims to (i) understand the computational foundation of generalization in novel scenarios, and (ii) build interactive systems that align with user’s goals.\n",
       "\n",
       "I am an Assistant Professor of Computer Science and Center for Data Science at New York University. I am affiliated with the CILVR Lab, the Machine Learning for Language Group, and the Alignment Research Group.\n",
       "\n",
       "Here are some directions I’m excited about nowadays:\n",
       "\n",
       "- Robustness: Machine learning models are trained on a fixed and often biased dataset, but face a constantly-changing world. How can we build predictors that align with human rationales, avoid spurious correlations, and generalize to out-of-distribution data? How can models adapt quickly given new information?\n",
       "- Truthfulness: We are increasingly relying on machine learning models (e.g., large language models) for critical tasks. How can we make sure that the model outputs conform to facts? Does the model know what it doesn’t know? Can it output a “proof” for its answer? How do we evaluate factuality efficiently for questions beyond the ability of an average person?\n",
       "- Human-AI collaboration: We want AI agents to deal with our daily minutiae, support our decision-making, and teach us complex concepts. How should the agent infer user intention and preferences, allow for fine-grained control, and take (natural language) feedback? How will this collaboration shape the future workforce?\n",
       "    \n",
       "\n",
       "## Recent Papers\n",
       "\n",
       "### **Title:** Language Models Learn to Mislead Humans via RLHF\n",
       "\n",
       "**Publish Date:** 2024-09-19\n",
       "\n",
       "**First Author:** Jiaxin Wen\n",
       "\n",
       "**Last Author:** Shi Feng\n",
       "\n",
       "**Middle Authors:** Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He\n",
       "\n",
       "**Abstract:** Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are wrong. We study this phenomenon under a standard RLHF pipeline, calling it \"U-SOPHISTRY\" since it is Unintended by model developers. Specifically, we ask time-constrained (e.g., 3-10 minutes) human subjects to evaluate the correctness of model outputs and calculate humans' accuracy against gold labels. On a question-answering task (QuALITY) and programming task (APPS), RLHF makes LMs better at convincing our subjects but not at completing the task correctly. RLHF also makes the model harder to evaluate: our subjects' false positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show that probing, a state-of-the-art approach for detecting Intended Sophistry (e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results highlight an important failure mode of RLHF and call for more research in assisting humans to align them.\n",
       "\n",
       "---------------\n",
       "\n",
       "\n",
       "### **Title:** Spontaneous Reward Hacking in Iterative Self-Refinement\n",
       "\n",
       "**Publish Date:** 2024-07-05\n",
       "\n",
       "**First Author:** Jane Pan\n",
       "\n",
       "**Last Author:** Shi Feng\n",
       "\n",
       "**Middle Authors:** He He, Samuel R. Bowman\n",
       "\n",
       "**Abstract:** Language models are capable of iteratively improving their outputs based on natural language feedback, thus enabling in-context optimization of user preference. In place of human users, a second language model can be used as an evaluator, providing feedback along with numerical ratings which the generator attempts to optimize. However, because the evaluator is an imperfect proxy of user preference, this optimization can lead to reward hacking, where the evaluator's ratings improve while the generation quality remains stagnant or even decreases as judged by actual user preference. The concern of reward hacking is heightened in iterative self-refinement where the generator and the evaluator use the same underlying language model, in which case the optimization pressure can drive them to exploit shared vulnerabilities. Using an essay editing task, we show that iterative self-refinement leads to deviation between the language model evaluator and human judgment, demonstrating that reward hacking can occur spontaneously in-context with the use of iterative self-refinement. In addition, we study conditions under which reward hacking occurs and observe two factors that affect reward hacking severity: model size and context sharing between the generator and the evaluator.\n",
       "\n",
       "---------------\n",
       "\n",
       "\n",
       "### **Title:** LLMs Are Prone to Fallacies in Causal Inference\n",
       "\n",
       "**Publish Date:** 2024-06-18\n",
       "\n",
       "**First Author:** Nitish Joshi\n",
       "\n",
       "**Last Author:** He He\n",
       "\n",
       "**Middle Authors:** Abulhair Saparov, Yixin Wang\n",
       "\n",
       "**Abstract:** Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks. However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize. Thus, this work investigates: Can LLMs infer causal relations from other relational data in text? To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations. We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.\n",
       "\n",
       "---------------\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "    ap.make_author_page(\"He He\", responses=he_papers, author_info=AUTHOR_INFO[\"He He\"], max_papers=50)\n",
    "))\n",
    "fu.dump_file(author_page, f\"{cfg.author_summaries_dir}he_he.markdown.v1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73133a-fa07-42f6-b8c1-695d5dfbf9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba607c11-d36f-4ab5-a97a-9eb28b181b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b926e4-44a6-4889-8229-4faef6cfcd84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316ed9e-3ceb-47d1-ad3e-3f5389c47f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_metadata_path(author):\n",
    "    assert author in AUTHORS\n",
    "    return f'{METADATA_DIR}{author.replace(\" \", \"_\")}_query_metadata.json'\n",
    "\n",
    "def get_author_metadata(author):\n",
    "    md_path = get_author_metadata_path(author)\n",
    "    md = fu.load_file(md_path)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b988a0c-3014-41c8-ad48-57005996e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_pdf_path(pdf_name):\n",
    "    return f'{PARSED_PDF_DIR}{pdf_name}.pkl'\n",
    "\n",
    "def get_parsed_pdf(pdf_name):\n",
    "    ppdf_path = get_parsed_pdf_path(pdf_name)\n",
    "    if os.path.exists(ppdf_path):\n",
    "        return fu.load_file(ppdf_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afc086-f402-47df-b1b6-2703aa2b116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_parsed_papers(author):\n",
    "    md = get_author_metadata(author)\n",
    "    pdfs_dict = md['pdfs_metadata']\n",
    "    parsed_pdfs_dict = []\n",
    "    for pdf_name, pdf_data in pdfs_dict.items():\n",
    "        ppdf = get_parsed_pdf(pdf_name)\n",
    "        if ppdf is None:\n",
    "            continue\n",
    "        ppdf_dict = {**pdf_data}\n",
    "        ppdf_dict['parsed_pdf'] = ppdf\n",
    "        parsed_pdfs_dict.append(ppdf_dict)\n",
    "    return parsed_pdfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3d56f-3a75-4a37-ad4a-6cf87a08d719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fd6f21-761e-4c03-8c9f-9e51b4a04ede",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Parsed, Extract Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff237-12ca-4e0d-9671-8cd97a09b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pdfs_pavel = get_author_parsed_papers(AUTHORS[0])\n",
    "print(f\">> Number of parsed papers for {AUTHORS[0]}: {len(parsed_pdfs_pavel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7c63b-e202-4bde-b72e-7d5da6dd2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppdf = parsed_pdfs_pavel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5569bc-dc4f-4f9c-be6c-a3d7f5d55a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_ppdf['title'])\n",
    "print(test_ppdf['published'])\n",
    "print(test_ppdf['authors'])\n",
    "print(test_ppdf['pdf_link'])\n",
    "print(f\">> Num blocks in parsed pdf: {len(test_ppdf['parsed_pdf'])}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c9cf06d-cd84-4506-9ba5-ecdcc4be3be2",
   "metadata": {},
   "source": [
    "for i, bl in enumerate(test_ppdf['parsed_pdf'][:5]):\n",
    "    print(f\">> Block: {i}\")\n",
    "    print(bl.text)\n",
    "    print(\"\\n\\n---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b932264-8ae6-4380-97df-06a52925e4d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e046df6-8721-4525-97b2-461708beb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_split_text(split_text, verbose=False):\n",
    "    buff = io.StringIO()\n",
    "    for section in split_text:\n",
    "        if verbose:\n",
    "            buff.write(f\"\\n\\n ===== Heading: {section['heading']} \\n\\n\")\n",
    "        buff.write(\"\\n\\n\".join(section['lines']))\n",
    "        buff.write(\"\\n\\n\")\n",
    "    return buff.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25328508-e9c5-4ca8-860b-9bc63aba038b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ab4ec-167b-46a1-a173-54458130d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_heading(text, title):\n",
    "    tls = text.split(\"\\n\")\n",
    "    title_str = f\"# {title}\"\n",
    "    sections = []\n",
    "\n",
    "    start_tl_strip = tls[0].strip()\n",
    "    if title_str in start_tl_strip or start_tl_strip[0] != \"#\":\n",
    "        start_heading = \"From Previous Block\"\n",
    "        start_lines = []\n",
    "    else:\n",
    "        start_heading = start_tl_strip[2:]\n",
    "        start_lines = [start_tl_strip]\n",
    "        \n",
    "    curr_section = {\"heading\": start_heading, \"lines\": start_lines}\n",
    "    for tl in tls[1:]:\n",
    "        tl_strip = tl.strip()\n",
    "        if len(tl_strip) == 0 or tl_strip[0].isdigit():\n",
    "            continue\n",
    "\n",
    "        if tl_strip[0] == \"#\":\n",
    "            # Drop all header mentions of the title, we'll add it back in\n",
    "            if title_str in tl_strip:\n",
    "                continue\n",
    "            # Otherwise start a new section\n",
    "            sections.append(curr_section)\n",
    "            curr_section = {\"heading\": tl_strip[2:], \"lines\": []}\n",
    "        curr_section['lines'].append(tl_strip)\n",
    "    \n",
    "    sections.append(curr_section)\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2c528-bed2-474b-8937-fc681ba463f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sects(input_ppdf, input_title):\n",
    "    all_sects = []\n",
    "    for i, block in enumerate(input_ppdf):\n",
    "        sects = split_by_heading(block.text, input_title)\n",
    "        if i == 0:\n",
    "            # Drop the title section\n",
    "            all_sects.extend(sects[1:])\n",
    "        else:\n",
    "            all_sects.extend(sects)\n",
    "    return all_sects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885add2-adb7-4bc2-b2c4-0d609b0bc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sections(sections):\n",
    "    grouped_sections = []\n",
    "\n",
    "    figs = []\n",
    "    last_was_fig = False\n",
    "    for section in sections:\n",
    "        if len(section['lines']) == 0:\n",
    "            continue\n",
    "            \n",
    "        heading = section['heading']\n",
    "        \n",
    "        # For ease of reading split the starting case out\n",
    "        if len(grouped_sections) == 0:\n",
    "            grouped_sections.append({\n",
    "                'heading': heading,\n",
    "                'lines': [],\n",
    "            })\n",
    "            \n",
    "        if heading.startswith(\"Figure\"):\n",
    "            figs.append(section)\n",
    "            last_was_fig = True\n",
    "            continue\n",
    "\n",
    "        if last_was_fig:\n",
    "            last_was_fig = False\n",
    "            if len(section['lines']) == 0:\n",
    "                print(section)\n",
    "                assert False\n",
    "            if len(section['lines'][0]) == 0:\n",
    "                print(section)\n",
    "                assert False\n",
    "            if section['lines'][0][0].islower():\n",
    "                first_l = f\"{section['heading']} {section['lines'][0]}\"\n",
    "                grouped_sections[-1]['lines'].append(first_l)\n",
    "                grouped_sections[-1]['lines'].extend(section['lines'][1:])\n",
    "                continue\n",
    "        \n",
    "        if (heading != \"From Previous Block\" and\n",
    "            grouped_sections[-1]['heading'] != heading\n",
    "        ):\n",
    "            grouped_sections.append({\n",
    "                'heading': heading,\n",
    "                'lines': [],\n",
    "            })\n",
    "        grouped_sections[-1]['lines'].extend(section['lines'])    \n",
    "    return grouped_sections, figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960080-b716-4293-872b-2fab5b5ed7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppdf_to_body_refs_figs(input_ppdf):\n",
    "    all_s = get_all_sects(input_ppdf['parsed_pdf'], input_ppdf['title'])\n",
    "    print(f\">> There are {len(all_s)} sections total.\")\n",
    "\n",
    "    grouped_s, figs_s = group_sections(all_s)\n",
    "    print(f\">> There are {len(grouped_s)} grouped sections and {len(figs_s)} figures.\")\n",
    "\n",
    "    body_s = []\n",
    "    references = None\n",
    "    for s in grouped_s:\n",
    "        if 'References' in s['heading']:\n",
    "            references = s\n",
    "            break\n",
    "        body_s.append(s)\n",
    "    return body_s, figs_s, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0dfb6-b9a6-4a40-90e9-9b088ba530b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "337933b1-c2e4-477e-8165-b95337bc128c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Full Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537ac0b-1978-4788-96b2-9b80635da4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_s, fg_s, rfs = ppdf_to_body_refs_figs(test_ppdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9ad8a-c897-40a7-ad5b-8e5f098a3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reconstruct_split_text(bd_s + fg_s))\n",
    "# rfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0cd93-a201-4f69-a0b5-80865047f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt in bd_s:\n",
    "    print(f\"{len(gt['lines']):2} | {gt['heading']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c57118-1487-490d-96e7-4ce730fb0f72",
   "metadata": {},
   "source": [
    "### Sub Section Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3d39f-9016-40ef-8fae-367f6fb140d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_test = get_all_sects(test_ppdf['parsed_pdf'], test_ppdf['title'])\n",
    "print(f\">> There are {len(all_sections_test)} sections total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b6f70-43e9-4bc1-8d5f-0d23d3c354f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_test, figs_test = group_sections(all_sections_test)\n",
    "print(f\">> There are {len(grouped_test)} grouped sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b964db-3d75-4789-8193-b471a61dd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt in grouped_test:\n",
    "    print(f\"{len(gt['lines']):3} | {gt['heading']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef9303-028d-4159-aec1-8e4425f7435c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d74c3b-05ac-43e8-a58e-9bc30529bba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91a50b-9fd0-4731-9af6-fb8d21d1553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_author_page(author):\n",
    "    bio = AUTHOR_INFO[author]\n",
    "    \n",
    "    buff = io.StringIO()\n",
    "    buff.write(f\"# Research Summary for {author}\\n\\n\")\n",
    "    buff.write(f\"## Bio\\n{bio}\\n\\n\")\n",
    "    \n",
    "\n",
    "    buff.write(\"## Recent Papers\\n\\n\")\n",
    "    parsed_pdfs_author = get_author_parsed_papers(author)\n",
    "    for ppdf in parsed_pdfs_author:\n",
    "        buff.write(f\"# Title: {ppdf['title']}\\n Published: {ppdf['published']}\\n\")\n",
    "        buff.write(\"Authors: \" + \", \".join(ppdf['authors']) + \"\\n\\n\")\n",
    "\n",
    "        bd_s, fg_s, rfs = ppdf_to_body_refs_figs(ppdf)\n",
    "        buff.write(reconstruct_split_text(bd_s + fg_s))# + [rfs]))\n",
    "        buff.write(f\"\\n\\n -------------- End Paper: {ppdf['title']}\")\n",
    "    return buff.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d69b8-ce90-4582-8a40-98225ed26b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_and_one_paper = make_author_page(AUTHORS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb898865-80ae-4204-975f-e32d10b90158",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu.dump_file(bio_and_one_paper, '/Users/daniellerothermel/drotherm/data/pavel_izmailov_summary_markdown.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4720f-192e-4405-9b4f-a56bf39b18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in AUTHORS:\n",
    "    bio_and_one_paper = make_author_page(author)\n",
    "    fu.dump_file(bio_and_one_paper, f'/Users/daniellerothermel/drotherm/data/{author.replace(\" \", \"_\").lower()}_summary_markdown.txt', verbose=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650845e-e4aa-48fe-a920-7321c777f348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
